{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7298ab15",
   "metadata": {},
   "source": [
    "# Architectures For Big Data\n",
    "## First Challenge\n",
    "\n",
    "### Valerio Cislaghi (mat. 982330)\n",
    "### Leonardo Menti (mat. 982296)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc89a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import add\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f37e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'CompID'\n",
    "value = 'Milliseconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a1c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './logDataset.csv'\n",
    "rdd = sc.textFile(path)\n",
    "\n",
    "header = rdd.first() \n",
    "\n",
    "# removing header (it could be better, because it makes a comparison with all the rows)\n",
    "rdd = rdd.filter(lambda x: x != header) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc264a40",
   "metadata": {},
   "source": [
    "### MEDIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d643831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': '076333', 'median': 611.0},\n",
       " {'key': '038137', 'median': 547.0},\n",
       " {'key': '031989', 'median': 552.0},\n",
       " {'key': '022692', 'median': 605.0},\n",
       " {'key': '009780', 'median': 128.0},\n",
       " {'key': '058073', 'median': 363.0},\n",
       " {'key': '072699', 'median': 373.0},\n",
       " {'key': '042031', 'median': 421.0},\n",
       " {'key': '065296', 'median': 827.0},\n",
       " {'key': '054531', 'median': 507.0},\n",
       " {'key': '089836', 'median': 659.0},\n",
       " {'key': '083436', 'median': 607.0},\n",
       " {'key': '050279', 'median': 455.0},\n",
       " {'key': '084215', 'median': 905.0},\n",
       " {'key': '026654', 'median': 747.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getItemMedian(keys, key, value): \n",
    "    key_i = keys.index(key)\n",
    "    value_i = keys.index(value)\n",
    "    def getItemAux(x):\n",
    "        row = x.split(',')\n",
    "        return (row[key_i], [float(row[value_i])])        \n",
    "    return getItemAux\n",
    "\n",
    "def calcMean(row):\n",
    "    key = row[0]\n",
    "    values = row[1]\n",
    "    median = values[int(len(values)/2)]\n",
    "    return {\"key\": key, \"median\": median}\n",
    "\n",
    "# we use getItemMedian to format the data\n",
    "# we delete empty values (and keys)\n",
    "# we concat elements with the same key\n",
    "# we sort the set\n",
    "# and finally we take a sample of 15 elements with a lazy approach\n",
    "\n",
    "median = rdd\\\n",
    "    .map(getItemMedian(header.split(','), key, value))\\\n",
    "    .filter(lambda x: x[0] != '' and x[1][0] != '')\\\n",
    "    .reduceByKey(list.__add__)\\\n",
    "    .mapValues(sorted)\\\n",
    "    .map(calcMean)\\\n",
    "    .take(15)\n",
    "\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04078f58",
   "metadata": {},
   "source": [
    "### MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83224d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': '076333', 'mean': 506.6},\n",
       " {'key': '038137', 'mean': 578.4},\n",
       " {'key': '031989', 'mean': 543.6},\n",
       " {'key': '022692', 'mean': 623.0},\n",
       " {'key': '009780', 'mean': 194.4},\n",
       " {'key': '058073', 'mean': 405.6},\n",
       " {'key': '072699', 'mean': 384.8},\n",
       " {'key': '042031', 'mean': 493.2},\n",
       " {'key': '065296', 'mean': 674.8},\n",
       " {'key': '054531', 'mean': 609.0},\n",
       " {'key': '089836', 'mean': 636.4},\n",
       " {'key': '083436', 'mean': 689.6},\n",
       " {'key': '050279', 'mean': 540.4},\n",
       " {'key': '084215', 'mean': 685.8},\n",
       " {'key': '026654', 'mean': 626.6}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/01 22:26:57 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 4063561 ms exceeds timeout 120000 ms\n",
      "22/02/01 22:26:57 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "def getItemMean(keys, key, value):\n",
    "    key_i = keys.index(key)\n",
    "    value_i = keys.index(value)\n",
    "    def getItemAux(x):\n",
    "        row = x.split(',')\n",
    "        return (row[key_i], (float(row[value_i]), 1))        \n",
    "    return getItemAux\n",
    "\n",
    "def calcMean(row):\n",
    "    key = row[0]\n",
    "    tot = row[1][0]\n",
    "    count = row[1][1]\n",
    "    return {\"key\": key, \"mean\": tot/count}\n",
    "\n",
    "# we use getItemMedian to format the data\n",
    "# we delete empty values (and keys)\n",
    "# we sum with a reduce function all the values like (x1, x2) (y1, y2) -> (x1 + y1, x2 + y2)\n",
    "# we calculate the mean like (tot, count) -> tot/count\n",
    "# and finally we take a sample of 15 elements with a lazy approach\n",
    "\n",
    "mean = rdd\\\n",
    "    .map(getItemMean(header.split(','), key, value))\\\n",
    "    .filter(lambda x: x[0] != '' and x[1] != '')\\\n",
    "    .reduceByKey(lambda acc, it: (acc[0] + it[0], acc[1] + it[1]))\\\n",
    "    .map(calcMean)\\\n",
    "    .take(15)\n",
    "    #.collect()\n",
    "\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa00ac",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18497593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confronts the sample obtained with mean/median calculated with pandas to prove the correctness of the program\n",
    "\n",
    "data = pd.read_csv('./logDataset.csv', dtype={key: str, value: float}) \n",
    "data = data[[key, value]]\n",
    "\n",
    "# mean test\n",
    "for row in mean:\n",
    "    spark_mean = row['mean']\n",
    "    pd_mean = data[data[key] == row['key']].groupby(key).mean()[value][0]\n",
    "    if pd_mean != row['mean']: raise Exception(f\"pandas mean is different from spark mean {pd_mean} != {spark_mean}\")\n",
    "\n",
    "# median test\n",
    "for row in median:\n",
    "    spark_median = row['median']\n",
    "    pd_median = data[data[key] == row['key']].groupby(key).median()[value][0]\n",
    "    if pd_median != row['median']: raise Exception(f\"pandas median is different from spark median {pd_median} != {spark_median}\")\n",
    "\n",
    "print(\"**** CORRECT ****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "data.groupby(key).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median\n",
    "data.groupby(key).median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
